title: Literatre Review
layout: default
permalink: /csce645/literature-review

## Literature Review
---
### Boundary Smoothing and Simplification 

Boundary smoothing and simplification has been extensively studied both in the fields of computer vision and geometric modeling. Techniques that use geometric methods for smoothing generally focus on simplifying the boundary curves into smoother curves that can still preserve the general shape of the objects or the original curves. Earlier methods on boundary/line simplification methods were developed for cartographic applications, owing to the high degree of curvature associated with the shapes of land masses, water bodies, etc. One of the seminal works in smoothing lines was introduced by Douglas et al., [[3](https://www.utpjournals.press/doi/abs/10.3138/fm57-6770-u75u-7727)]. They developed the popular Douglas-Peucker algorithm to reduce the number of points required to represent a line. The algorithm first identifies the point farthest from the line segment between the end points of the given line, and then checks if the distance between this point (i.e., farthest point), is greater than/lesser than epsilon (user defined distance parameter). If it is lesser than epsilon, all the points that are not originally marked to keep are removed; and if the distance is greater than epsilon, the farthest point is kept and the process is repeated recursively till a simplified curve is obtained. Another seminal work on line simplification was proposed by Wang et al., [[4](https://www.tandfonline.com/doi/abs/10.1559/152304098782441750)], where they developed a knowledge based system focused on the cartographic domain, to encode the rules that a cartographer might use to simplify the lines. In their method they first extract the bends present in the curves and then identify the bend attributes such as size and shape of bends. Then, they perform various operations such as elimination or exageration on the bends determined by the knowledge-based rules, to obtain smoother lines. While this method focuses on using domain knowledge to generate rules for operating on the line bends, methods using geometric-based rules could be better generalized to other applications. An important geometric technique that motivated prior and more recent works is the identification of dominant points present on the boundaries of shapes, i.e. points with high degree of curvature, containing important shape information. One such prior work by Garcia et al., [[1](https://www.sciencedirect.com/science/article/pii/0098300494900469)], achieved boundary simplification, by first segmenting the boundary curves into multiple segments, each of which highlighting specific features of the shape, and then finding the dominant points within these line segments. They finally determine the specific degree of simplification for each segment by using Gaussian smoothing, and minimizing the  normalized measure of zeros of curvature of the segments. Garrido et al., [[2](https://www.sciencedirect.com/science/article/pii/S0031320397001040)] similarly focused on simplification of boundries using a multi-scale dominant point detection method, combined with linear interpolation between the detected points to form simpler boundaries and therefore shapes. They obtained a multi-scale representation of the shape based on maxima trajectories through scale space through which they get the dominant points which they then use to linearly interpolate betwen to obtain the boundary of the shape.

Another common approach to boundary simplification is the polygonal approximation of the boundary [[24](https://www.sciencedirect.com/science/article/pii/S0167865510001947?casa_token=cfOUhQPyQJ4AAAAA:QlLaLlcq11H2TtgeCq9KEbTJ7snMK8kz1VwbuwueX0t5DKJBjFg6O4IYtPvdOjNgMlzOf3LGKQo),[25](https://www.sciencedirect.com/science/article/pii/S0031320306003761?casa_token=nV3nhpo6oOUAAAAA:-3kZfJSeivyyTT03bms2jl4-sdyejB780GTvijkTlx55Asj44A3KYTRVK6tF7ZO1XB2spLk6Yw4)]. Prior seminal works include the work done by Ramer [[5](https://www.sciencedirect.com/science/article/pii/S0146664X72800170)], where they presented an itrative procedure to obtain polygonal approximation of 2D curves. The algorithm approximates the 2D curves as polygon with small number of edges. The method involves iteratively selecting a subset of the points constituting the 2D curve/boundary as the vertices of a polygon such that the distance from the curve to the polygon satisfies a given tolerance. More recent approaches, such as the work by Poyato et al., [[6](https://www.sciencedirect.com/science/article/pii/S0031320309002532?casa_token=oi8Guh87ZpsAAAAA:ZuHgsIiqNqQUmMKQ9Yuqnvd5N-v4YoNuJWNj5uXBrgqw9KyUUR74ZBe6NYIR_5kq2sj05xgMeew)], combines the polygonal approximation approach with that of dominant point detection. In their work, they approximate polygons of planar curves through specialized techniques of finding the dominant points of the curve. First, they find the dominant points from the break points of the initial boundary, where the integral square is zero. To obtain these dominant points, they first delete most of the original break points by suppressing ones whose perpendicular distance to a straight line is lower than a variable threshold value, thus signifying that the points are quasi-collinear to the previous and next break points. The removal of the redundant points is continued till a user-defined termination condition is met, which takes into consideration the decrease in the length of the contour and the error. According to their results, a threshold value of 0.5 produced the most precise contours. Ai et al., [[22](https://www.tandfonline.com/doi/pdf/10.1080/13658816.2016.1197399?needAccess=true)] also proposed a way for simplification of polylines by generating an envelope structure around the polyline using Delaunay triangulation. In their approach, they propose a computational method of implementing Perkal's proposal [23] of having a circle of diameter epsilon roll on both sides of the polyline in order to identify and remove small bend features in the polyline. In their implementation they simulate the effect of the rolling circle using Delaunay Triangulation to construct an envelope structure similar to that generated by the rolling circle. They then develop a strategy to determine the best connection method within the envelope to simplify the polyline while maitaining its general shape and preventing self-intersection.

While the focus of my work in this project will be to explore the geometric approach for boundary simplification, the image-processing-based methods can certainly be a source of motivation. Quite a few image-processing based methods exist to smooth out edges and boundaries detected in images. These methods are typically performed on binary/gray-scale images. Saint-Marc et al.,[[7](https://www.computer.org/csdl/journal/tp/1991/06/i0514/13rRUxASuNX)] introduced their seminal work on Adaptive Smoothing of signal, in the form of intensity image, range image or planar curves. Their method involves repeatedly convolving the signal with a very small averaging mask that is weighted by a measure of continuity of the signal at each point. They derive a new scale-space representation of the signal using an adaptive smoothing parameter *k* as the scale dimension. Changing this parameter value, therefore, allows multiscale smoothing of the signal. More recently, Lopez-Molina et al.,[[8](https://www.sciencedirect.com/science/article/pii/S0950705113000464?casa_token=AK9hCpms51cAAAAA:gdufitrrtNhC__Tkv1OCW5LQmI4nuc3GBWyuZqUjj4WVDjr-pzj8uP5lkld1NEg3QwAVRmgJcBE)] proposed a multiscale method for edge detection based on increasing Gaussian smoothing, Sobel operators and coarse-to-fine edge tracking. Their method samples a finite number of images from Gaussian smoothing and then applies the Sobel method on each of them. They then use a novel coarse-to-fine tracking algorithm in order to combine the robustness and accuracy in the location of edges. Their method is able to perform better than the single scale method (Sobel) that it is originally based on. 

The above geometric and image-based methods provide a wide range of ways to smooth the boundaries of objects or edges detected in images. However, semantically segmented images provide a richer source of information, i.e., the semantic labels associated with each segment or region in the image, which can be better used by analyzing the shapes associated with the different labels. One such technique that enables this is skeletonization, which is discussed in detail in the following section.

### Skeletonization Techniques

![Skeletonization](\assets\images\skel_voronoi.png) *Examples of the Hierarchical Voronoi Skeletons generated in [[9](https://www.sciencedirect.com/science/article/pii/003132039400105U)]*

Skeletonization is an effective way of analyzing shapes of objects. Geometrically, skeletonization is the process of transforming a 2D or 3D object into a line representation. The seminal work by Olgniewicz et al., [[9](https://www.sciencedirect.com/science/article/pii/003132039400105U)] introduced hierarchic voronoi skeletons, where they first regularized voronoi diagrams to first obtain basic skeleton structures of the object, and then developed a hierarchic organization of the skeleton constituents. In order to first generate the voronoi skeletons, they attributed each edges of the voronoi diagram, constructed using the discrete boundary points of the object, with a residual function and then extracted a subset of the components of the voronoi diagram to approximately extract the medial axis transformation of the object. Next, they established a hierarchy of the skeleton constituents, names skeleton pyramid, by traversing along the voronoi skeleton and rank ordering the skeleton constituents based on the value of rotation of the constituents about given vertices. Parameters associated with this process were used to obtain skeletons of different resolutions. However, this technique of cleaning-up the skeleton (pruning), did not guarantee topology preservation. Bai et al., [[10](https://ieeexplore.ieee.org/abstract/document/4069261?casa_token=Ge-1Q7E_7_EAAAAA:h2-Ka0b8-Fw9n7V4N5HqJB054XEaudE32Qvh22SOe_BQ3YHG4oEQ4t7y2Y3uQL_qaYLZBhHroA)] introduced a skeleton pruning method that was based on contour partitioning and could preserve the topology of the original skeleton while preventing spurious branches from being produced. They specifically used Discrete Curve Evolution (DCE) to partition the contour. As any digital curve can be regarded as a polygon, DCE is used to simplify the shape of the contour by iteratively removing a vertex from one of the polygons whose relevance measure to the contour is the smallest. Through this approach, we can get partitions or subarcs of the object contours. The skeleton can then be pruned by removing skeleton branches whose generating points are on the same contour subarc, which can translate into removing redundant skeleton branches and retaining the necessary visual branches. Skeleton pruning has thus become an important step in generating accurate and stable skeletons of shapes and objects. Skeleton pruning can also be used to recreate smoother versions of the original shape, as shown by Tek et al.,[[11](https://link.springer.com/article/10.1023/A:1011229911541)]. They proposed a shape smoothing method based on the analysis of the underlying medial axis of the shape and then iteratively removing the skeletal branches of the shape to obtain a smoother shape. Their method of skeleton pruning focuses on removing the instability of skeleton deformations through symmetry transformations, where the transform is applied to all symmetry branches, which may result in more appropriate smoothing of the shapes. They call this transform the splice transform, which prunes a branch, merges the two segments of the base branch into one and then makes appropriate corrections to the coupled skeletal branches. This method allows them to construct and preserve coarse-scale curvature extrema. 

Apart from the medial axis transformation methods, other traditional methods also explored the use of the grassfire transform to generate skeletons. Leymarie et al., [[13](https://www.computer.org/csdl/journal/tp/1992/01/i0056/13rRUygT7fZ)] simulated the grassfire transform using an active contour model. Their method integrated the use of the features defined by the region and the boundary of the object. They implemented the 2D dynamic grassfire technique that relied on a euclidean transform that was combined with an active contour model (eg. snake) to minimize an energy function. This along with the curvature features of the boundary allowed them to extract a Euclidean Skeleton Representation of the the shape. Their approach was able to bypass the discretization problems that were associated with other traditional skeletonization algortihms.

Along with geometric methods, image processing-based skeletonization techniques also exist, generally called thinning, which reduce objects in binary images into a single pixel wide representation [[26](http://www.uel.br/pessoal/josealexandre/stuff/thinning/ftp/lam-lee-survey.pdf)]. Prior works include the simple thinning algorithms for gray-scale pictures introduced by Dyer et al., [[12](https://ieeexplore.ieee.org/abstract/document/4766880?casa_token=jkOlXMbXFlYAAAAA:gaZA30O6UTw8cAre_kxnNfxrWpjpVM7pLgAZsXvtmgbFdhg-8AbVujYpKbWG2oAFs89D4c6NsQ)]. Through their method, elongated black objects in gray-scale images could be thinned into arcs/curves that maintained the connectedness. They did this by repeatedly deleting black border points as long as their deletion did not affect the local connectedness of the black points in their neighborhoods. Their definition of connectedness in their approach referred to the presence of a path joining two points on which no point was lighter that either of them. More recently, the work by Jerripothula et al., [[14](https://ieeexplore.ieee.org/abstract/document/8099896?casa_token=2WLXDzIkLmwAAAAA:Q441gyiKMI6hqaj3mRElEw2Cq3nTfz0TXJt3w-3WcN5Jc2Z7RbLvhLYf6C5mAZahmheAXtUEuA)], introduced the concept of object co-skeletonization with co-segmentation. The co-skeletonization aspect of the paper explores joint skeleton extraction of common objects present in a set of semantically similar images. The use of multiple images helps in developing skeletons of objects in the real world by hoping that a commonness prior exists across the similar images. Skeletons can typically be used for segmentation tasks, and skeletons themselves need good segmentation, as such, a coupled framework of segmentation and skeletonization was proposed. Using their approach, shape simplification is also possible based on the simplification of the obtained skeletons. 

A wide variety of work also exists in obtaining the skeleton representations of 3D shapes and models. Ju et al. [[20](https://www.sciencedirect.com/science/article/pii/S0010448507000450?casa_token=UWht7QGfd94AAAAA:bCj2mcgOz543KtyYHa-qSbJeeLWUiM9EHx8lxUa5VZuBodk3wEgCApEj7Ah6V8cWQvGqOdhFo9U)] presented a method for computing skeletons of volumetric models by alternating thinning and a novel skeleton pruning routine. Using this method, they are able to create skeletons that are parameterized by two user-specified numbers that are used to determine the size of curve and surface features present on the skeleton. Lien et al., [[21](https://dl.acm.org/doi/abs/10.1145/1128888.1128919?casa_token=sBIZWA29q7UAAAAA:6KrvSilWRg2F9SnamypIWqfPCBRCWrV9HO-hPSHKsS1YCTw5fPBNf8y43PSQDdPzZXOM07pkG7EeXA)] proposed a different approach to skeletonization of 3D shapes, where they simultaneously generated hierarchical shape decomposition and a set of multi-resolution skeletons. In their approach, the skeleton was extracted from the components of the shape decomposition and as such were both interdependent. The iterative process of shape decomposition and skeletonization is continued till the quality of the skeleton becomes satisfactory. They used the principal axis of the convex hull of a component of the decomposed shape to generate the skeletons of the 3D shapes, and then evaluated the skeleton by comparing it to the components itself.  

The general improvements in the skeletonization algorithms have enabled the creation of datasets focusing on the skeletal representations of objects. One such dataset is the SkelNetOn 2019 dataset [[15](https://openaccess.thecvf.com/content_CVPRW_2019/papers/SkelNetOn/Demir_SkelNetOn_2019_Dataset_and_Challenge_on_Deep_Learning_for_Geometric_CVPRW_2019_paper.pdf)] that provides three different representations of the objects and skeletons: Pixel SkelNetOn (pixel-based representation), Point SkelNetOn (point cloud representation) and Parametric SkelNetOn (parametric-curve representations). Several recent works have developed deep learning models to train on this dataset [[16](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w16/Dey_Subpixel_Dense_Refinement_Network_for_Skeletonization_CVPRW_2020_paper.pdf), [17](https://openaccess.thecvf.com/content_CVPRW_2019/papers/SkelNetOn/Nathan_SkeletonNet_Shape_Pixel_to_Skeleton_Pixel_CVPRW_2019_paper.pdf), [18](https://openaccess.thecvf.com/content_CVPRW_2019/papers/SkelNetOn/Atienza_Pyramid_U-Network_for_Skeleton_Extraction_From_Shape_Points_CVPRW_2019_paper.pdf)]. Dey [[16](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w16/Dey_Subpixel_Dense_Refinement_Network_for_Skeletonization_CVPRW_2020_paper.pdf)] proposed a skeleton extraction architecture called the Subpixel Dense Refinement Network, which consisted of a three-stage encoder-decoder network with dense interconnections between the decoder networks of each stage. The architecture using subpixel convolutions instead of upsampling layers and transposed convolutional layers in order to minimize information loss during upsampling of the encoded features. The architecture achieved an F1-score of 0.7708 on the validation set of the Pixel SkelNetOn dataset. Nathan et al.,[[17](https://openaccess.thecvf.com/content_CVPRW_2019/papers/SkelNetOn/Nathan_SkeletonNet_Shape_Pixel_to_Skeleton_Pixel_CVPRW_2019_paper.pdf)] similarly developed a U-net model for the same dataset. Their model included an encoder-decoder structure where the decoder was designed in the format of HED (Holistically-nested edge detection) architecture [[19](https://openaccess.thecvf.com/content_iccv_2015/papers/Xie_Holistically-Nested_Edge_Detection_ICCV_2015_paper.pdf)], consisting of 4 side layers fused into one dilation convolutional layer to connect the broken links of the generated skeletons. Their approach also achieved an F1 score of 0.77.  Atienza [[18](https://openaccess.thecvf.com/content_CVPRW_2019/papers/SkelNetOn/Atienza_Pyramid_U-Network_for_Skeleton_Extraction_From_Shape_Points_CVPRW_2019_paper.pdf)] on the other hand, developed a Pyramid U-Network for skeleton extraction from shape points using the Point SkelNetOn for training and testing. The architecture consists of a pyramid of three U-Nets that can predict the skeleton from a give shape point cloud. The rapid growth in the field of machine learning and deep learning will introduce a wide variety of architectures that can very quickly generate/predict skeletons for specific tasks. 

The work that will be done in this project will certainly be influenced by a lot of the works discussed above, specifically the methods that enable shape smoothing through skeleton pruning. That being said, most methods introduced here focus on smoothing boundaries and generating skeletons for a single class of objects or shapes, and do not discuss much about the presence and influence of multiple classes of shapes within the image. The work in this project will try to fill the knowledge gap of implementing modified versions of these techniques to multi-class semantically segmented images. While the skeleton generation might be done on an individual class based method, techniques such as pruning and smoothing of the shapes will be attempted on a multi-class basis.

[Link to Home Page](https://sjvyas.github.io/csce645/)

### References
[1] Garcia, J. A., and J. Fdez-Valdivia. "Boundary simplification in cartography preserving the characteristics of the shape features." _Computers & Geosciences_ 20.3 (1994): 349-368.

[2] Garrido, A., and M. Garcia-Silvente. "Boundary simplification using a multiscale dominant-point detection algorithm." _Pattern Recognition_ 31.6 (1998): 791-804.

[3] Douglas, David H., and Thomas K. Peucker. "Algorithms for the reduction of the number of points required to represent a digitized line or its caricature." _Cartographica: the international journal for geographic information and geovisualization_ 10.2 (1973): 112-122.

[4] Wang, Zeshen, and Jean-Claude Müller. "Line generalization based on analysis of shape characteristics." _Cartography and Geographic Information Systems_ 25.1 (1998): 3-15.

[5] Ramer, Urs. "An iterative procedure for the polygonal approximation of plane curves." _Computer graphics and image processing_ 1.3 (1972): 244-256.

[6] Carmona-Poyato, A., et al. "Polygonal approximation of digital planar curves through break point suppression." _Pattern Recognition_ 43.1 (2010): 14-25.

[7] Saint-Marc, Philippe, Jer-Sen Chen, and Gérard Medioni. "Adaptive smoothing: A general tool for early vision." _IEEE Transactions on Pattern Analysis & Machine Intelligence_ 13.06 (1991): 514-529.

[8] Lopez-Molina, Carlos, et al. "Multiscale edge detection based on Gaussian smoothing and edge tracking." _Knowledge-Based Systems_ 44 (2013): 101-111.

[9] Ogniewicz, Robert L., and Olaf Kübler. "Hierarchic voronoi skeletons." _Pattern recognition_ 28.3 (1995): 343-359.

[10] Bai, Xiang, Longin Jan Latecki, and Wen-Yu Liu. "Skeleton pruning by contour partitioning with discrete curve evolution." _IEEE transactions on pattern analysis and machine intelligence_ 29.3 (2007): 449-462.

[11] Tek, Hüseyin, and Benjamin B. Kimia. "Boundary smoothing via symmetry transforms." _Journal of Mathematical Imaging and Vision_ 14.3 (2001): 211-223.

[12] Dyer, Charles R., and Azriel Rosenfeld. "Thinning algorithms for gray-scale pictures." _IEEE Transactions on Pattern Analysis and Machine Intelligence_ 1 (1979): 88-89.

[13] Leymarie, Frédéric, and Martin D. Levine. "Simulating the grassfire transform using an active contour model." _IEEE Transactions on Pattern Analysis & Machine Intelligence_ 14.01 (1992): 56-75.

[14] Jerripothula, Koteswar Rao, et al. "Object co-skeletonization with co-segmentation." _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 2017.

[15] Demir, Ilke, et al. "Skelneton 2019: Dataset and challenge on deep learning for geometric shape understanding." _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_. 2019.

[16] Dey, Sohom. "Subpixel Dense Refinement Network for Skeletonization." _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_. 2020.

[17] Nathan, Sabari, and Priya Kansal. "Skeletonnet: Shape pixel to skeleton pixel." _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_. 2019.

[18] Atienza, Rowel. "Pyramid U-network for skeleton extraction from shape points." _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_. 2019.

[19] Xie, Saining, and Zhuowen Tu. "Holistically-nested edge detection." _Proceedings of the IEEE international conference on computer vision_. 2015.

[20] Ju, Tao, Matthew L. Baker, and Wah Chiu. "Computing a family of skeletons of volumetric models for shape description." _Computer-Aided Design_ 39.5 (2007): 352-360.

[21] Lien, Jyh-Ming, John Keyser, and Nancy M. Amato. "Simultaneous shape decomposition and skeletonization." _Proceedings of the 2006 ACM symposium on Solid and physical modeling_. 2006.

[22] Ai, Tinghua, et al. "Envelope generation and simplification of polylines using Delaunay triangulation." _International Journal of Geographical Information Science_ 31.2 (2017): 297-319.

[23] Perkal, Julian. "An attempt at objective generalization." _Michigan Inter-University Community of Mathematical Geographers, Discussion Paper_ 10 (1966).

[24] Parvez, Mohammad Tanvir, and Sabri A. Mahmoud. "Polygonal approximation of digital planar curves through adaptive optimizations." _Pattern Recognition Letters_ 31.13 (2010): 1997-2005.

[25] Kolesnikov, Alexander, and Pasi Fränti. "Polygonal approximation of closed discrete curves." _Pattern Recognition_ 40.4 (2007): 1282-1293.

[26] Lam, Louisa, Seong-Whan Lee, and Ching Y. Suen. "Thinning methodologies-a comprehensive survey." _IEEE Transactions on pattern analysis and machine intelligence_ 14.9 (1992): 869-885.
